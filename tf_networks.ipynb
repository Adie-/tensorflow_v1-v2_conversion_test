{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow_train.utils.data_format import get_channel_index\n",
    "from tensorflow_train.layers.layers import conv2d, avg_pool2d, concat_channels\n",
    "from tensorflow_train.layers.interpolation import upsample2d_linear\n",
    "from tensorflow_train.layers.initializers import he_initializer\n",
    "from tensorflow_train.networks.unet import UnetClassic2D\n",
    "from tensorflow_train.networks.unet_base import UnetBase\n",
    "\"\"\"\n",
    "Code adapted from\n",
    "https://github.com/christianpayer/MedicalDataAugmentationTool/blob/master/bin/experiments/localization/hand_xray/network.py\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def network_scn(input, num_landmarks, is_training,\n",
    "                data_format='channels_last'):\n",
    "    num_filters = 128\n",
    "    local_kernel_size = [5, 5]\n",
    "    spatial_kernel_size = [15, 15]\n",
    "    downsampling_factor = 8\n",
    "    padding = 'same'\n",
    "    kernel_initializer = he_initializer\n",
    "    activation = tf.nn.relu\n",
    "    heatmap_initializer = tf.truncated_normal_initializer(stddev=0.0001)\n",
    "    local_activation = None\n",
    "    spatial_activation = None\n",
    "    with tf.variable_scope('local_appearance'):\n",
    "        node = conv2d(input, num_filters, kernel_size=local_kernel_size,\n",
    "                      name='conv1', activation=activation,\n",
    "                      kernel_initializer=kernel_initializer, padding=padding,\n",
    "                      data_format=data_format, is_training=is_training)\n",
    "        node = conv2d(node, num_filters, kernel_size=local_kernel_size,\n",
    "                      name='conv2', activation=activation,\n",
    "                      kernel_initializer=kernel_initializer, padding=padding,\n",
    "                      data_format=data_format, is_training=is_training)\n",
    "        node = conv2d(node, num_filters, kernel_size=local_kernel_size,\n",
    "                      name='conv3', activation=activation,\n",
    "                      kernel_initializer=kernel_initializer, padding=padding,\n",
    "                      data_format=data_format, is_training=is_training)\n",
    "        local_heatmaps = conv2d(node, num_landmarks,\n",
    "                                kernel_size=local_kernel_size,\n",
    "                                name='local_heatmaps',\n",
    "                                activation=local_activation,\n",
    "                                kernel_initializer=heatmap_initializer,\n",
    "                                padding=padding, data_format=data_format,\n",
    "                                is_training=is_training)\n",
    "    with tf.variable_scope('spatial_configuration'):\n",
    "        local_heatmaps_downsampled = avg_pool2d(local_heatmaps,\n",
    "                                                [downsampling_factor,\n",
    "                                                 downsampling_factor],\n",
    "                                                name='local_heatmaps_downsampled',\n",
    "                                                data_format=data_format)\n",
    "        channel_axis = get_channel_index(local_heatmaps_downsampled,\n",
    "                                         data_format)\n",
    "        local_heatmaps_downsampled_split = tf.split(local_heatmaps_downsampled,\n",
    "                                                    num_landmarks,\n",
    "                                                    channel_axis)\n",
    "        spatial_heatmaps_downsampled_split = []\n",
    "        for i in range(num_landmarks):\n",
    "            local_heatmaps_except_i = tf.concat([local_heatmaps_downsampled_split[j]for j in range(num_landmarks) if i != j],\n",
    "                                                name='h_app_except_'+str(i),\n",
    "                                                axis=channel_axis)\n",
    "            h_acc = conv2d(local_heatmaps_except_i, 1,\n",
    "                           kernel_size=spatial_kernel_size,\n",
    "                           name='h_acc_'+str(i), activation=spatial_activation,\n",
    "                           kernel_initializer=heatmap_initializer,\n",
    "                           padding=padding, data_format=data_format,\n",
    "                           is_training=is_training)\n",
    "            spatial_heatmaps_downsampled_split.append(h_acc)\n",
    "        spatial_heatmaps_downsampled = tf.concat(spatial_heatmaps_downsampled_split,\n",
    "                                                 name='spatial_heatmaps_downsampled',\n",
    "                                                 axis=channel_axis)\n",
    "        spatial_heatmaps = upsample2d_linear(spatial_heatmaps_downsampled,\n",
    "                                             [downsampling_factor,\n",
    "                                              downsampling_factor],\n",
    "                                             name='spatial_prediction',\n",
    "                                             padding='valid_cropped',\n",
    "                                             data_format=data_format)\n",
    "    with tf.variable_scope('combination'):\n",
    "        heatmaps = local_heatmaps * spatial_heatmaps\n",
    "    return heatmaps\n",
    "\n",
    "\n",
    "def network_downsampling(input, num_landmarks, is_training,\n",
    "                         data_format='channels_last'):\n",
    "    num_filters = 128\n",
    "    kernel_size = [5, 5]\n",
    "    num_levels = 3\n",
    "    padding = 'same'\n",
    "    kernel_initializer = he_initializer\n",
    "    activation = tf.nn.relu\n",
    "    heatmap_initializer = tf.truncated_normal_initializer(stddev=0.0001)\n",
    "    heatmap_activation = None\n",
    "    node = input\n",
    "    with tf.variable_scope('downsampling'):\n",
    "        for i in range(num_levels):\n",
    "            with tf.variable_scope('level' + str(i)):\n",
    "                node = conv2d(node, num_filters, kernel_size=kernel_size,\n",
    "                              name='conv0', activation=activation,\n",
    "                              kernel_initializer=kernel_initializer,\n",
    "                              padding=padding, data_format=data_format,\n",
    "                              is_training=is_training)\n",
    "                node = conv2d(node, num_filters, kernel_size=kernel_size,\n",
    "                              name='conv1', activation=activation,\n",
    "                              kernel_initializer=kernel_initializer,\n",
    "                              padding=padding, data_format=data_format,\n",
    "                              is_training=is_training)\n",
    "                if i != num_levels - 1:\n",
    "                    node = avg_pool2d(node, [2, 2], name='downsampling',\n",
    "                                      data_format=data_format)\n",
    "        heatmaps = conv2d(node, num_landmarks, kernel_size=[1, 1],\n",
    "                          name='heatmaps', activation=heatmap_activation,\n",
    "                          kernel_initializer=heatmap_initializer,\n",
    "                          padding=padding, data_format=data_format,\n",
    "                          is_training=is_training)\n",
    "    return heatmaps\n",
    "\n",
    "\n",
    "def network_conv(input, num_landmarks, is_training,\n",
    "                 data_format='channels_last'):\n",
    "    num_filters = 128\n",
    "    kernel_size = [11, 11]\n",
    "    padding = 'same'\n",
    "    kernel_initializer = he_initializer\n",
    "    activation = tf.nn.relu\n",
    "    heatmap_initializer = tf.truncated_normal_initializer(stddev=0.0001)\n",
    "    heatmap_activation = None\n",
    "    node = input\n",
    "    with tf.variable_scope('downsampling'):\n",
    "        node = conv2d(node, num_filters, kernel_size=kernel_size, name='conv0',\n",
    "                      activation=activation,\n",
    "                      kernel_initializer=kernel_initializer, padding=padding,\n",
    "                      data_format=data_format, is_training=is_training)\n",
    "        node = conv2d(node, num_filters, kernel_size=kernel_size, name='conv1',\n",
    "                      activation=activation,\n",
    "                      kernel_initializer=kernel_initializer, padding=padding,\n",
    "                      data_format=data_format, is_training=is_training)\n",
    "        node = conv2d(node, num_filters, kernel_size=kernel_size, name='conv2',\n",
    "                      activation=activation,\n",
    "                      kernel_initializer=kernel_initializer, padding=padding,\n",
    "                      data_format=data_format, is_training=is_training)\n",
    "        node = conv2d(node, num_filters, kernel_size=kernel_size, name='conv3',\n",
    "                      activation=activation,\n",
    "                      kernel_initializer=kernel_initializer, padding=padding,\n",
    "                      data_format=data_format, is_training=is_training)\n",
    "        node = conv2d(node, num_filters, kernel_size=kernel_size, name='conv4',\n",
    "                      activation=activation,\n",
    "                      kernel_initializer=kernel_initializer, padding=padding,\n",
    "                      data_format=data_format, is_training=is_training)\n",
    "        node = conv2d(node, num_filters, kernel_size=kernel_size, name='conv5',\n",
    "                      activation=activation,\n",
    "                      kernel_initializer=kernel_initializer, padding=padding,\n",
    "                      data_format=data_format, is_training=is_training)\n",
    "        heatmaps = conv2d(node, num_landmarks, kernel_size=[1, 1],\n",
    "                          name='heatmaps', activation=heatmap_activation,\n",
    "                          kernel_initializer=heatmap_initializer,\n",
    "                          padding=padding, data_format=data_format,\n",
    "                          is_training=is_training)\n",
    "    return heatmaps\n",
    "\n",
    "\n",
    "def network_unet(input, num_landmarks, is_training,\n",
    "                 data_format='channels_last'):\n",
    "    num_filters = 128\n",
    "    num_levels = 4\n",
    "    padding = 'same'\n",
    "    kernel_initializer = he_initializer\n",
    "    activation = tf.nn.relu\n",
    "    heatmap_initializer = tf.truncated_normal_initializer(stddev=0.0001)\n",
    "    heatmap_activation = None\n",
    "    with tf.variable_scope('unet'):\n",
    "        unet = UnetClassic2D(num_filters, num_levels, activation=activation,\n",
    "                             kernel_initializer=kernel_initializer,\n",
    "                             data_format=data_format, padding=padding)\n",
    "        node = unet(input, is_training=is_training)\n",
    "        heatmaps = conv2d(node, num_landmarks, kernel_size=[1, 1],\n",
    "                          name='heatmaps', activation=heatmap_activation,\n",
    "                          kernel_initializer=heatmap_initializer,\n",
    "                          padding=padding, data_format=data_format,\n",
    "                          is_training=is_training)\n",
    "    return heatmaps\n",
    "\n",
    "\n",
    "class UnetClassicAvgLinear2D(UnetBase):\n",
    "    def combine(self, parallel_node, upsample_node, current_level,\n",
    "                is_training):\n",
    "        return concat_channels([parallel_node, upsample_node],\n",
    "                               name='concat' + str(current_level),\n",
    "                               data_format=self.data_format)\n",
    "\n",
    "    def contracting_block(self, node, current_level, is_training):\n",
    "        node = self.conv(node, current_level, '0', is_training)\n",
    "        node = self.conv(node, current_level, '1', is_training)\n",
    "        return node\n",
    "\n",
    "    def parallel_block(self, node, current_level, is_training):\n",
    "        return node\n",
    "\n",
    "    def expanding_block(self, node, current_level, is_training):\n",
    "        node = self.conv(node, current_level, '0', is_training)\n",
    "        node = self.conv(node, current_level, '1', is_training)\n",
    "        return node\n",
    "\n",
    "    def downsample(self, node, current_level, is_training):\n",
    "        return avg_pool2d(node, [2, 2], name='downsample' + str(current_level),\n",
    "                          data_format=self.data_format)\n",
    "\n",
    "    def upsample(self, node, current_level, is_training):\n",
    "        return upsample2d_linear(node, [2, 2], name='upsample' +\n",
    "                                 str(current_level),\n",
    "                                 data_format=self.data_format)\n",
    "\n",
    "    def conv(self, node, current_level, postfix, is_training):\n",
    "        return conv2d(node,\n",
    "                      self.num_filters(current_level),\n",
    "                      [3, 3],\n",
    "                      name='conv' + postfix,\n",
    "                      activation=self.activation,\n",
    "                      normalization=None,\n",
    "                      is_training=is_training,\n",
    "                      data_format=self.data_format,\n",
    "                      kernel_initializer=self.kernel_initializer,\n",
    "                      padding=self.padding)\n",
    "\n",
    "\n",
    "def network_scn_mmwhs(input, num_landmarks, is_training,\n",
    "                      data_format='channels_last'):\n",
    "    downsampling_factor = 8\n",
    "    num_filters = 128\n",
    "    num_levels = 4\n",
    "    spatial_kernel_size = [5, 5]\n",
    "    kernel_initializer = he_initializer\n",
    "    activation = tf.nn.relu\n",
    "    local_kernel_initializer = tf.truncated_normal_initializer(stddev=0.0001)\n",
    "    local_activation = tf.nn.tanh\n",
    "    spatial_kernel_initializer = tf.truncated_normal_initializer(stddev=0.0001)\n",
    "    spatial_activation = None\n",
    "    padding = 'reflect'\n",
    "    with tf.variable_scope('unet'):\n",
    "        unet = UnetClassicAvgLinear2D(num_filters, num_levels,\n",
    "                                      data_format=data_format,\n",
    "                                      double_filters_per_level=False,\n",
    "                                      kernel_initializer=kernel_initializer,\n",
    "                                      activation=activation, padding=padding)\n",
    "        local_prediction = unet(input, is_training=is_training)\n",
    "        local_prediction = conv2d(local_prediction, num_landmarks, [1, 1],\n",
    "                                  name='local_prediction', padding=padding,\n",
    "                                  kernel_initializer=local_kernel_initializer,\n",
    "                                  activation=local_activation,\n",
    "                                  is_training=is_training)\n",
    "    with tf.variable_scope('spatial_configuration'):\n",
    "        local_prediction_pool = avg_pool2d(local_prediction,\n",
    "                                           [downsampling_factor] * 2,\n",
    "                                           name='local_prediction_pool')\n",
    "        scconv = conv2d(local_prediction_pool, num_filters,\n",
    "                        spatial_kernel_size, name='scconv0',\n",
    "                        padding=padding, kernel_initializer=kernel_initializer,\n",
    "                        activation=activation, is_training=is_training)\n",
    "        scconv = conv2d(scconv, num_filters, spatial_kernel_size,\n",
    "                        name='scconv1', padding=padding,\n",
    "                        kernel_initializer=kernel_initializer,\n",
    "                        activation=activation, is_training=is_training)\n",
    "        scconv = conv2d(scconv, num_filters, spatial_kernel_size,\n",
    "                        name='scconv2', padding=padding,\n",
    "                        kernel_initializer=kernel_initializer,\n",
    "                        activation=activation, is_training=is_training)\n",
    "        spatial_prediction_pool = conv2d(scconv, num_landmarks,\n",
    "                                         spatial_kernel_size,\n",
    "                                         name='spatial_prediction_pool',\n",
    "                                         padding=padding,\n",
    "                                         kernel_initializer=spatial_kernel_initializer,\n",
    "                                         activation=spatial_activation,\n",
    "                                         is_training=is_training)\n",
    "        spatial_prediction = upsample2d_linear(spatial_prediction_pool,\n",
    "                                               [downsampling_factor] * 2,\n",
    "                                               name='spatial_prediction',\n",
    "                                               padding='valid_cropped')\n",
    "    with tf.variable_scope('combination'):\n",
    "        prediction = local_prediction * spatial_prediction\n",
    "    return prediction\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
